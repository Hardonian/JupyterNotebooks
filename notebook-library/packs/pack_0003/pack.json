{
  "id": "pack_0003",
  "slug": "prompt-evaluation-harness",
  "title": "Prompt Evaluation Harness",
  "version": "1.0.0",
  "tool": "jupyter",
  "key_type": "notebook",
  "category": "ai_ops",
  "difficulty": "advanced",
  "runtime_minutes": 20,
  "description": "Evaluate and score prompts with drift detection and performance metrics",
  "tags": [
    "prompt-engineering",
    "evaluation",
    "metrics",
    "ai"
  ],
  "inputs": [
    {
      "name": "prompts",
      "type": "json",
      "required": false,
      "path": "data/prompts.json",
      "notes": "Prompt data (will use sample if not provided)"
    }
  ],
  "outputs": [
    {
      "name": "evaluation_results",
      "type": "json",
      "path": "outputs/evaluation_results.json",
      "notes": "Evaluation scores and metrics"
    },
    {
      "name": "evaluation_report",
      "type": "md",
      "path": "outputs/evaluation_report.md",
      "notes": "Summary report"
    }
  ],
  "entrypoint": "main.ipynb",
  "python": {
    "min": "3.10",
    "tested": [
      "3.11"
    ]
  },
  "install": {
    "tool": "pip",
    "command": "pip install -r requirements.txt"
  },
  "run": {
    "command": "jupyter nbconvert --to notebook --execute main.ipynb --output executed.ipynb"
  },
  "license": {
    "spdx": "GPL-3.0-only",
    "file": "LICENSE.txt"
  },
  "changelog": "CHANGELOG.md",
  "assets": {
    "cover": "assets/cover.png",
    "flow": "assets/flow.svg",
    "preview_html": "assets/preview.html"
  }
}